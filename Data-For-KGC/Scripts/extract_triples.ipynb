{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdflib\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54aa1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import RDFS\n",
    "from urllib.parse import urldefrag\n",
    "from rdflib.term import BNode,Literal\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb896cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract triples from turtle or ttl file (ontology)\n",
    "def get_triples(graph):\n",
    "    def get_frag(node):\n",
    "        if '#' in node:\n",
    "            return urldefrag(node)[1]\n",
    "        else:\n",
    "            return node\n",
    "    def get_english(node,graph):\n",
    "      if node in renameURIs:\n",
    "        return renameURIs[node]\n",
    "      elif (node,RDFS.label,None) in graph:\n",
    "        labelNode = [x[2] for x in graph.triples((node,RDFS.label,None))][0]\n",
    "        if isinstance(labelNode,Literal):\n",
    "          return labelNode.value\n",
    "        else:\n",
    "          return labelNode\n",
    "      elif (isinstance(node,Literal)):\n",
    "\n",
    "        return node.value\n",
    "      elif '#' in node.toPython():\n",
    "        return get_frag(node.toPython())\n",
    "\n",
    "\n",
    "    def change_case(string):\n",
    "        \n",
    "        if string is not None:\n",
    "            string = string.replace(\"-\", \"\")\n",
    "            string = string.replace(\" \", \"_\")\n",
    "            if string:\n",
    "                res = [string[0].lower()]\n",
    "                for c in string[1:]:\n",
    "                    if c in ('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "                        res.append('_')\n",
    "                        res.append(c.lower())\n",
    "                    else:\n",
    "                        res.append(c)\n",
    "\n",
    "                return ''.join(res)\n",
    "            return string\n",
    "\n",
    "\n",
    "    triples = []\n",
    "    for s, p, o in graph:\n",
    "      #out.write(\"\\t\".join([s,p,o]) +\"\\n\")\n",
    "      if isinstance(s,BNode) or isinstance(p,BNode) or isinstance(o,BNode) or  (p in (prefixURI,RDFS.label,contributorURI,licenseURI,commentURI,noteURI,definitionURI,deprecatedURI,seeAlsoURI,scopeNoteURI)) or (p == typeURI and o in (classURI,namedIndividualURI)) or (s == versionInfoURI or p == versionInfoURI or o ==versionInfoURI):\n",
    "        continue\n",
    "      s_name = change_case(get_english(s,graph))\n",
    "      p_name = change_case(get_english(p,graph))\n",
    "      o_name = change_case(get_english(o,graph))\n",
    "\n",
    "\n",
    "      if s_name is None or p_name is None or o_name is None:\n",
    "        continue\n",
    "      else:\n",
    "        triples.append([s_name,p_name,o_name])\n",
    "    return triples\n",
    "\n",
    "def init_graph(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        graph = rdflib.Graph()\n",
    "        graph.parse(f,format=\"ttl\")\n",
    "        \n",
    "        triples = get_triples(graph)\n",
    "    \n",
    "        return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24627675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"CRISPsubset.ttl\"\n",
    "\n",
    "tri1 = init_graph(filename)\n",
    "\n",
    "triples = list(set(tuple(sorted(sub)) for sub in tri1))\n",
    "\n",
    "df = pd.DataFrame(tri)\n",
    "df.to_csv(\"combined.tsv\",index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaf6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second script to extract triples from ttl files (ontology)\n",
    "from rdflib import Graph\n",
    "graph = Graph()\n",
    "graph.parse('./doce.ttl', format='ttl')\n",
    "\n",
    "s_list = []\n",
    "p_list = []\n",
    "o_list = []\n",
    "for s, p, o in graph:\n",
    "    s = s.split(\"/\")[-1]\n",
    "    s = s.split(\"#\")[-1]\n",
    "    s_list.append(s)\n",
    "    \n",
    "    p = p.split(\"/\")[-1]\n",
    "    p = p.split(\"#\")[-1]\n",
    "    p_list.append(p)\n",
    "    \n",
    "    o = o.split(\"/\")[-1]\n",
    "    o = o.split(\"#\")[-1]\n",
    "    o_list.append(o)\n",
    "\n",
    "print(len(s_list))\n",
    "\n",
    "\n",
    "new_s_list = []\n",
    "for index,item in enumerate(s_list):\n",
    "    new_s_list.append(change_case(item))\n",
    "\n",
    "new_p_list = []\n",
    "for index,item in enumerate(p_list):\n",
    "    new_p_list.append(change_case(item))\n",
    "\n",
    "\n",
    "new_o_list = []\n",
    "for index,item in enumerate(o_list):\n",
    "    new_o_list.append(change_case(item))\n",
    "\n",
    "\n",
    "with open(\"train.txt\", \"w\") as train:\n",
    "    for index, item in enumerate(new_s_list[0:640]):\n",
    "        line = new_s_list[index] + \"\\t\" + new_p_list[index] + \"\\t\" + new_o_list[index] + \"\\n\"\n",
    "        \n",
    "        train.write(line) \n",
    "\n",
    "with open(\"test.txt\", \"w\") as test:\n",
    "    index = 640 \n",
    "    for item in new_s_list[640:]:\n",
    "        line = new_s_list[index] + \"\\t\" + new_p_list[index] + \"\\t\" + new_o_list[index] + \"\\n\"\n",
    "        index +=1\n",
    "        \n",
    "        test.write(line)\n",
    "        \n",
    "        \n",
    "entities = set(new_s_list) \n",
    "for item in new_o_list:\n",
    "    entities.add(item)\n",
    "    \n",
    "print(len(entities))\n",
    "\n",
    "relations = set(new_p_list)\n",
    "print(len(relations))\n",
    "\n",
    "\n",
    "with open(\"entities.txt\", \"w\") as file:\n",
    "    for item in entities:\n",
    "        line = item +  \"\\n\"\n",
    "        \n",
    "        file.write(line) \n",
    "\n",
    "with open(\"relations.txt\", \"w\") as file:\n",
    "    for item in relations:\n",
    "        line = item +  \"\\n\"\n",
    "        \n",
    "        file.write(line) \n",
    "\n",
    "new_s_list1 = []\n",
    "for index,item in enumerate(s_list):\n",
    "    item = item.replace(\"-\", \" \")\n",
    "    new_s_list1.append(item)\n",
    "\n",
    "new_p_list1 = []\n",
    "for index,item in enumerate(p_list):\n",
    "    item = item.replace(\"-\", \" \")\n",
    "    new_p_list1.append(item)\n",
    "\n",
    "\n",
    "new_o_list1 = []\n",
    "for index,item in enumerate(o_list):\n",
    "    item = item.replace(\"-\", \" \")\n",
    "    new_o_list1.append(item)\n",
    "    \n",
    "\n",
    "entities = set(new_s_list1) \n",
    "for item in new_o_list1:\n",
    "    entities.add(item)\n",
    "\n",
    "relations = set(new_p_list1)\n",
    "    \n",
    "    \n",
    "with open(\"entitiestotext.txt\", \"w\") as file:\n",
    "    for item in entities:\n",
    "        line = item +  \"\\n\"\n",
    "        file.write(line) \n",
    "\n",
    "with open(\"relationstotext.txt\", \"w\") as file:\n",
    "    for item in relations:\n",
    "        line = item +  \"\\n\"\n",
    "        file.write(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf786ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts relation instances that is predicates and entities form triples data\n",
    "triples = []\n",
    "def extract_rel_instances(data):\n",
    "    \n",
    "    p_list = set()\n",
    "    e_list = set()\n",
    "\n",
    "    for index,rows in data.iterrows():\n",
    "        try:\n",
    "            s_name = rows[0].replace(\" \", \"_\")\n",
    "            p_name = rows[1].replace(\" \", \"_\")\n",
    "            o_name = rows[2].replace(\" \", \"_\")\n",
    "            p_list.add(p_name)\n",
    "            e_list.add(s_name)\n",
    "            e_list.add(o_name)\n",
    "\n",
    "\n",
    "            triples.append([s_name,p_name,o_name])\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "\n",
    "    return p_list, e_list, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dad11f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows relations and their correnponding number of occurances in the triple\n",
    "def show_relations_instances(p_list,e_list, triples):\n",
    "    count_list = []\n",
    "    for predicate in p_list:\n",
    "        count = 0\n",
    "        for item in triples:\n",
    "            if item[1] == predicate:\n",
    "                count +=1 \n",
    "        count_list.append(count)\n",
    "\n",
    "    count_dict = []\n",
    "    p_list = list(p_list)\n",
    "\n",
    "    for index,item in enumerate(count_list):\n",
    "        count_dict.append([p_list[index], item])\n",
    "\n",
    "    print(len(count_dict))  \n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(count_dict, columns=[\"Relationship\", \"Number of instances\"])\n",
    "    return df,count_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17af4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters relations with instances greater than a threshold amount. Eg: triples with relations occuring in more than 10 triples.\n",
    "def filter_relations(count_dict, triples, threshold):\n",
    "    new_rels = set()\n",
    "    for key, value in count_dict:\n",
    "        if value > threshold:\n",
    "            new_rels.add(key)\n",
    "\n",
    "    print(new_rels)\n",
    "    newtriples = []\n",
    "    new_ent = set()\n",
    "    for s,p,o in triples:\n",
    "        if p in new_rels:\n",
    "            newtriples.append([s,p,o])\n",
    "            new_ent.add(s)\n",
    "            new_ent.add(o)\n",
    "    return newtriples, new_ent, new_rels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03288b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves data in a file\n",
    "def save_data(filename, data):\n",
    "    with open (filename,\"w\") as file:\n",
    "        for item in data:\n",
    "            file.write(item)\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "def save_data_text(filename, data):\n",
    "    with open (filename,\"w\") as file:\n",
    "        for item in data:\n",
    "            text = item + \"\\t\" + item.replace(\"_\", \" \")\n",
    "            file.write(text)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cffdd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits dataset into train, test and dev, using a threshold value\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6778b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts entities from a data\n",
    "def extract_entities(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        entity = []\n",
    "        data = f.readlines()\n",
    "        for d in data:\n",
    "            d=d.split(\"\\n\")[0]\n",
    "            entity.append(d)\n",
    "            \n",
    "    return entity\n",
    "\n",
    "#Extract the current triples from the data\n",
    "def current_triples(data):\n",
    "    curr = []\n",
    "    for index,rows in data.iterrows():\n",
    "        s = rows[0]\n",
    "        p = rows[1]\n",
    "        o = rows[2]\n",
    "        curr.append([s,p,o])\n",
    "    return curr\n",
    "    \n",
    "#Creates negative examples of data for triple classification task with entities present in the data\n",
    "def create_neg_examples(data, entities):\n",
    "    triples = []\n",
    "    curr = current_triples(data)\n",
    "    for index,rows in data.iterrows():\n",
    "        s = rows[0]\n",
    "        p = rows[1]\n",
    "        o = rows[2]\n",
    "        \n",
    "        checkpoint = True\n",
    "        while checkpoint:\n",
    "            neg_o = random.choice(entities)\n",
    "            if [s,p,neg_o] not in curr and [neg_o, p, o] not in curr:\n",
    "                triples.append([s,p,o, 1])\n",
    "                if random.choice(range(0,20000)) % 2 == 0:\n",
    "                    triples.append([s,p,neg_o, -1])\n",
    "                else:\n",
    "                    triples.append([neg_o,p,o, -1])\n",
    "                checkpoint = False\n",
    "            \n",
    "    return triples\n",
    "\n",
    "#Creates negative examples for triple classificaiton task and saves in csv destination file.\n",
    "def create_neg_eg_and_save(source, destination, entsource):\n",
    "    entities = extract_entities(entsource)\n",
    "    data = pd.read_csv(source, sep=\"\\t\",header=None)\n",
    "    triples = create_neg_examples(data, entities)\n",
    "    df = pd.DataFrame(triples)\n",
    "    df.to_csv(destination,index=False,header=False,sep=\"\\t\")\n",
    "\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Triples from the predicates file extracted from SemMedDB\n",
    "\n",
    "from langdetect import DetectorFactory, detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def get_triples_umls(filename, req_list):\n",
    "    triples = []\n",
    "    data = pd.read_csv(filename, sep=\",\")\n",
    "    for index,rows in data.iterrows():\n",
    "        one = str(rows[0])\n",
    "        two = str(rows[1])\n",
    "        three = str(rows[2])\n",
    "\n",
    "        try:\n",
    "            if detect(one) == \"en\" and detect(two) == \"en\" and detect(three) == \"en\":\n",
    "                s_name = one.lower().replace(\" \", \"_\")\n",
    "                p_name = two.lower().replace(\" \", \"_\")\n",
    "                o_name = three.lower().replace(\" \", \"_\")\n",
    "                triples.append([s_name,p_name,o_name])\n",
    "\n",
    "        except LangDetectException:\n",
    "            continue\n",
    "    return triples\n",
    "\n",
    "import re\n",
    "\n",
    "def replace_and_return_string(string):\n",
    "    replacing_strings=[\" \", \"__\"]\n",
    "    string = re.sub(r\"^\\s+|\\s+$\", \"\", string)\n",
    "    string = re.sub('[^a-zA-Z0-9 \\n\\.]', '_', string)\n",
    "    string = string.replace(\".\",\"\")\n",
    "    for chars in replacing_strings:\n",
    "        string = string.replace(chars, \"_\")\n",
    "    \n",
    "    return string\n",
    "\n",
    "    \n",
    "def get_triples_umls1(filename, req_list):\n",
    "    triples = []\n",
    "    data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    for index,rows in data.iterrows():\n",
    "        one = str(rows[0])\n",
    "        two = str(rows[1])\n",
    "        three = str(rows[2])\n",
    "        s_name = replace_and_return_string(one.lower())\n",
    "        p_name = replace_and_return_string(two.lower())\n",
    "        o_name = replace_and_return_string(three.lower())\n",
    "        triples.append([s_name,p_name,o_name])\n",
    "    return triples\n",
    "\n",
    "triples1 = get_triples_umls1(\"<FileName>\", [])\n",
    "\n",
    "df3 = pd.DataFrame(triples1)\n",
    "df3.to_csv(\"<TriplesFile>.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "data1 = pd.read_csv(\"<TriplesFile>.tsv\", sep=\"\\t\")\n",
    "\n",
    "p_list, e_list, triples = extract_rel_instances(data1)\n",
    "new_ent = e_list\n",
    "new_rels = p_list\n",
    "\n",
    "print(len(triples))\n",
    "df,count_dict = show_relations_instances(p_list, e_list, triples1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ba1976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves formatted entities and relations with their corresponding texts.\n",
    "save_data(\"<DataFolder>/entities.txt\", new_ent)\n",
    "save_data(\"<DataFolder>/relations.txt\", new_rels)\n",
    "\n",
    "save_data_text(\"<DataFolder>/entity2text.txt\", new_ent)\n",
    "save_data_text(\"<DataFolder>/relation2text.txt\", new_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts For Relation Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de2426b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a relation dictionary to combine triples with that relation instance\n",
    "def combine_relation_instances(p_list,e_list, triples):\n",
    "    reldict = {}\n",
    "    for predicate in p_list:\n",
    "        itemslist = []\n",
    "        for item in triples:\n",
    "            if item[1] == predicate:\n",
    "                itemslist.append(item)  \n",
    "                \n",
    "        reldict[predicate] = itemslist\n",
    "    return reldict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines the relation instances, splits triples for each realtion into train, test and dev such that the datasets cover all types of relations.\n",
    "\n",
    "relations = combine_relation_instances(p_list, e_list, triples1)\n",
    "\n",
    "count =  0\n",
    "for key,value in relations.items():\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(value)\n",
    "    \n",
    "    train, validate, test= train_validate_test_split(df, train_percent=.8, validate_percent=.1)\n",
    "    train.to_csv(\"<DataFolder>/for-rel/train.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "    validate.to_csv(\"<DataFolder>/for-rel/dev.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "    test.to_csv(\"<DataFolder>/for-rel/test.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "    count += len(value)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required for LINK and RELATION prediction. Shuffles data in the dataset. \n",
    "\n",
    "def shuffle_data_and_save(file):\n",
    "    data = pd.read_csv(file, sep=\"\\t\")\n",
    "    df = data.sample(frac=1)\n",
    "    df.to_csv(file,index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "18eb3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data_and_save(\"<DataFolder>/for-rel/train.tsv\") \n",
    "shuffle_data_and_save(\"<DataFolder>/for-rel/test.tsv\") \n",
    "shuffle_data_and_save(\"<DataFolder>/for-rel/dev.tsv\")\n",
    "\n",
    "#Note: You need to copy the entities, entity2text, relations, and relation2text to the <DataFolder>/for-rel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to filter relations with count greater than a given threshold the following pieces of code are useful else jump directly to Link classification scripts.\n",
    "#Filtering RELATIONS \n",
    "#STEP 1\n",
    "\n",
    "new_triples, new_ent, new_rels = filter_relations(count_dict, triples1, 100) #Here threshold is given 100\n",
    "print(len(new_rels))\n",
    "df,count_dict = show_relations_instances(new_rels, new_ent, new_triples)\n",
    "print(df)\n",
    "\n",
    "\n",
    "df_umls = pd.DataFrame(new_triples)\n",
    "df_umls.to_csv(\"<DataFolder>/filtered/final.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "save_data(\"<DataFolder>/filtered/entities.txt\", new_ent)\n",
    "save_data(\"<DataFolder>/filtered/relations.txt\", new_rels)\n",
    "\n",
    "save_data_text(\"<DataFolder>/filtered/entity2text.txt\", new_ent)\n",
    "save_data_text(\"<DataFolder>/filtered/relation2text.txt\", new_rels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d602b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2\n",
    "#For Relation - Viewing subject| object - with each unique relation\n",
    "\n",
    "data = pd.read_csv(\"<DataFolder>/final.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "p_list = set()\n",
    "e_list = set()\n",
    "dictKey = defaultdict(list)\n",
    "\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0].lower()\n",
    "    p = rows[1].lower()\n",
    "    o = rows[2].lower()\n",
    "    \n",
    "    p_list.add(p)\n",
    "    e_list.add(s)\n",
    "    e_list.add(o)\n",
    "    key = s+ \"|\" + o\n",
    "    dictKey[key].append(p)\n",
    "        \n",
    "\n",
    "trip1 = []\n",
    "maxV = 0\n",
    "for key, value in dictKey.items():\n",
    "    maxV = max(maxV,len(value))\n",
    "    trip1.append([key, value, len(value)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b306b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the subject/object realtion data in a file.\n",
    "df2 = pd.DataFrame(trip1)\n",
    "df2.to_csv(\"<DataFolder>/filtered/so-r.tsv\",index=False,header=[\"subject | object\", \"relations\", \"count\"],sep=\"\\t\")\n",
    "print(maxV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3\n",
    "#Extracting new triples from the filtered list\n",
    "out_trip = defaultdict(list)\n",
    "overall = []\n",
    "count = 0 \n",
    "for rows in trip1:\n",
    "    if rows[2] > 1:\n",
    "        s, o = rows[0].split(\"|\")\n",
    "        for p in rows[1]:\n",
    "            trip = [s,p,o]\n",
    "            if trip in new_triples: \n",
    "                count +=1\n",
    "                overall.append(trip)\n",
    "                out_trip[rows[0]].append(trip)\n",
    "                new_triples.remove(trip)\n",
    "\n",
    "print(\"Done\", count)\n",
    "print(len(new_triples), len(overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ad652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a train, test and validation splits from the new list of triples filtered with relations threshold.\n",
    "#STEP 4\n",
    "\n",
    "out_list = []\n",
    "for key, value in out_trip.items():\n",
    "    out_list.append(value)\n",
    "    \n",
    "n = len(out_list)\n",
    "id1 = int(n * 0.8)\n",
    "id2 = int(n * 0.8 * 0.1) + int(n * 0.8)\n",
    "\n",
    "train3 = []\n",
    "test3 = []\n",
    "validate3 = []\n",
    "count = 0\n",
    "\n",
    "for item in out_list:\n",
    "    if count >=0 and count < id1:\n",
    "        for val in item:\n",
    "            train3.append(val)\n",
    "    elif count >= id1 and count < id2:\n",
    "        for val in item:\n",
    "            test3.append(val)\n",
    "    elif count >=id2 and count < n:\n",
    "        for val in item:\n",
    "            validate3.append(val)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue for relation filtering  - Part2\n",
    "#STEP 5\n",
    "\n",
    "relations = combine_relation_instances(new_rels, new_ent, new_triples)\n",
    "count =  0\n",
    "\n",
    "train3 = pd.DataFrame(train3)\n",
    "validate3 = pd.DataFrame(validate3)\n",
    "test3 = pd.DataFrame(test3)\n",
    "\n",
    "for key,value in relations.items():\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(value)\n",
    "    \n",
    "    tr, v, te= train_validate_test_split(df, train_percent=.8, validate_percent=.1)\n",
    "    count += len(value)\n",
    "    \n",
    "    train3 = pd.concat([train3, tr])\n",
    "    validate3 = pd.concat([validate3, v])\n",
    "    test3 = pd.concat([test3, te])\n",
    "\n",
    "train3.to_csv(\"<DataFolder>/filtered/train.tsv\",index=False,header=False,sep=\"\\t\", mode='a')\n",
    "validate3.to_csv(\"<DataFolder>/filtered/dev.tsv\",index=False,header=False,sep=\"\\t\", mode='a')\n",
    "test3.to_csv(\"<DataFolder>/filtered/test.tsv\",index=False,header=False,sep=\"\\t\", mode='a')\n",
    "\n",
    "print(len(train3), len(test3), len(validate3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5c891893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP - 6\n",
    "shuffle_data_and_save(\"<DataFolder>/filtered/train.tsv\") \n",
    "shuffle_data_and_save(\"<DataFolder>/filtered/test.tsv\") \n",
    "shuffle_data_and_save(\"<DataFolder>/filtered/dev.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------\n",
    "#LINK PREDICTION \n",
    "#If you are not doing link prediction then skip the scripts to Triple Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "97e101a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts For Link Prediction\n",
    "#STEP 1\n",
    "\n",
    "#Creating dictionary of key (subject entity + relation) and objects occuring with the same key\n",
    "def view_link_relation_instances(p_list,e_list, triples):\n",
    "    reldict = {}\n",
    "    for item in triples:\n",
    "        reldict[item[0]+\"|\"+item[1]] = []\n",
    "    for item in triples:\n",
    "        reldict[item[0]+\"|\"+item[1]].append(item[2])\n",
    "    \n",
    "    return reldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "318c3678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#STEP 2\n",
    "reldict = view_link_relation_instances(p_list, e_list, triples1)\n",
    "count_dict = []\n",
    "\n",
    "for key, value in reldict.items():\n",
    "    count_dict.append([key, value, len(value)]) \n",
    "\n",
    "df2 = pd.DataFrame(count_dict)\n",
    "df2.to_csv(\"<DataFolder>/for-link/er.tsv\",index=False,header=[\"subject | relation\", \"tail entity\", \"number of instances\"],sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3\n",
    "\n",
    "new_trip = triples1.copy()\n",
    " \n",
    "out_trip = defaultdict(list)\n",
    "overall = []\n",
    "count = 0 \n",
    "\n",
    "for rows in count_dict:\n",
    "    if rows[2] > 1:\n",
    "        s, p = rows[0].split(\"|\")\n",
    "        for o in rows[1]:\n",
    "            trip = [s,p,o]\n",
    "            if trip in new_trip: \n",
    "                count +=1\n",
    "                overall.append(trip)\n",
    "                out_trip[rows[0]].append(trip)\n",
    "                new_trip.remove(trip)\n",
    "\n",
    "print(len(new_trip), len(overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4\n",
    "out_list = []\n",
    "for key, value in out_trip.items():\n",
    "    out_list.append(value)\n",
    "    \n",
    "n = len(out_list)\n",
    "id1 = int(n * 0.8)\n",
    "id2 = int(n * 0.8 * 0.1) + int(n * 0.8)\n",
    "\n",
    "train2 = []\n",
    "test2 = []\n",
    "validate2 = []\n",
    "count = 0\n",
    "\n",
    "for item in out_list:\n",
    "    if count >=0 and count < id1:\n",
    "        for val in item:\n",
    "            train2.append(val)\n",
    "    elif count >= id1 and count < id2:\n",
    "        for val in item:\n",
    "            test2.append(val)\n",
    "    elif count >=id2 and count < n:\n",
    "        for val in item:\n",
    "            validate2.append(val)\n",
    "    count +=1\n",
    "\n",
    "\n",
    "print(len(train2), len(test2), len(validate2))\n",
    "print(len(train2) + len(test2) + len(validate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "96343002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_link_relation_instances(p_list, e_list, triples):\n",
    "    linkdict = {}\n",
    "    for item in triples:\n",
    "        linkdict[item[0]+\"|\"+item[1]] = []\n",
    "    for item in triples:\n",
    "        linkdict[item[0]+\"|\"+item[1]].append(item)\n",
    "    return linkdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#STEP 5\n",
    "\n",
    "train2 = pd.DataFrame(train2)\n",
    "validate2 = pd.DataFrame(validate2)\n",
    "test2 = pd.DataFrame(test2)\n",
    "\n",
    "df = pd.DataFrame(new_trip)\n",
    "\n",
    "tr, v, te= train_validate_test_split(df, train_percent=.7, validate_percent=.15)\n",
    "count += len(value)\n",
    "\n",
    "\n",
    "train2 = pd.concat([train2, tr])\n",
    "validate2 = pd.concat([validate2, v])\n",
    "test2 = pd.concat([test2, te])\n",
    "\n",
    "train2.to_csv(\"<DataFolder>/for-link/train.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "validate2.to_csv(\"<DataFolder>/for-link/dev.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "test2.to_csv(\"<DataFolder>/for-link/test.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "\n",
    "print(len(train2), len(test2), len(validate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a3eaff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6\n",
    "shuffle_data_and_save(\"<DataFolder>/for-link/train.tsv\") \n",
    "shuffle_data_and_save(\"<DataFolder>for-link/test.tsv\") \n",
    "shuffle_data_and_save(\"<DataFolder>/for-link/dev.tsv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08799968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#TRIPLE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7542a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Triple classification \n",
    "\n",
    "train3, validate3, test3 = train_validate_test_split(df3, train_percent=.8, validate_percent=.1)\n",
    "            \n",
    "train3.to_csv(\"<DataFolder>/for-triple/train.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "validate3.to_csv(\"<DataFolder>/for-triple/dev1.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "test3.to_csv(\"<DataFolder>/for-triple/test1.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "\n",
    "create_neg_eg_and_save(\"<DataFolder>/for-triple/test1.tsv\", \n",
    "                       \"<DataFolder>/for-triple/test.tsv\",\n",
    "                      \"<DataFolder>/for-triple/entities.txt\")\n",
    "\n",
    "create_neg_eg_and_save(\"<DataFolder>/for-triple/dev1.tsv\", \n",
    "                       \"<DataFolder>/for-triple/dev.tsv\",\n",
    "                      \"<DataFolder>/for-triple/entities.txt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final UMLS Dataset Cleaning - Removing neg relations, inverse relations and same subject-object \n",
    "\n",
    "from collections import defaultdict\n",
    "data = pd.read_csv(\"<DataFolder>/combined.tsv\",header=None, low_memory=False, sep=\"\\t\")\n",
    "\n",
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0 \n",
    "p_list = set()\n",
    "e_list = set()\n",
    "triples = []\n",
    "dictKey = defaultdict(list)\n",
    "\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0].lower()\n",
    "    p = rows[1].lower()\n",
    "    o = rows[2].lower()\n",
    "    if s == o: \n",
    "        count +=1\n",
    "    elif [o,p,s] in triples:\n",
    "        count1 +=1\n",
    "    elif p.startswith(\"neg_\"):\n",
    "        count2 +=1\n",
    "#     elif [s,p,o] not in triples:\n",
    "    else:\n",
    "        triples.append([s,p,o])\n",
    "        p_list.add(p)\n",
    "        e_list.add(s)\n",
    "        e_list.add(o)\n",
    "        \n",
    "        \n",
    "        key = s+ \"|\" + o\n",
    "        dictKey[key].append(p)\n",
    "        \n",
    "         \n",
    "print(count, count1,count2, len(triples))\n",
    "  \n",
    "df,count_dict = show_relations_instances(p_list, e_list, triples)\n",
    "print(df)  \n",
    "\n",
    "df1 = pd.DataFrame(triples)\n",
    "df1.to_csv(\"<DataFolder>/init.tsv\",index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Supplement code for ranking triples and combining data triples from SemRepTool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Ranking tool\n",
    "\n",
    "#Converting tsv to txt file\n",
    "data = pd.read_csv(\"<PATH>/trip.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "data.to_csv(\"<PATH>/sub_rel_obj.txt\",header=None, sep=\" \", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating frequency score/ranking for subject object \n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('./stats.txt', sep='<>| ', header=None, skiprows=1,\n",
    "                 names=['sub', 'rel', 'obj', 'rank', 'stat', 'f012',\n",
    "                        'f0', 'f1', 'f2', 'f01', 'f02', 'f12'], engine='python')\n",
    "deg = pd.read_csv('<PATH>/concept_degree.txt', sep=' ', header=None, names=['cui', 'deg'])\n",
    "cui2deg = dict(zip(deg['cui'], deg['deg']))\n",
    "\n",
    "def lookup(cui):\n",
    "    try:\n",
    "        return cui2deg[cui]\n",
    "    except Exception as ex:\n",
    "        return 0\n",
    "\n",
    "df['sub_deg'] = df.apply(lambda row: lookup(row['sub']), axis=1)\n",
    "df['obj_deg'] = df.apply(lambda row: lookup(row['obj']), axis=1)\n",
    "\n",
    "\n",
    "names = ['stat', 'sub_deg', 'obj_deg']\n",
    "x = df[names].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_norm = min_max_scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_norm, columns=names, index = df.index)\n",
    "df[['stat_norm', 'sub_deg_norm', 'obj_deg_norm']] = df_temp\n",
    "df['score'] = df['stat_norm'] + df['sub_deg_norm'] + df['obj_deg_norm']\n",
    "\n",
    "df.to_csv('<PATH>/sub_rel_obj_freq_score.tsv', sep='\\t', header=None, columns=['sub', 'rel', 'obj', 'f012', 'score'],\n",
    "          index=False, float_format='%.15f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting max frequency\n",
    "data = pd.read_csv(\"<PATH>/concept_degree.txt\",header=None, sep=\" \", low_memory=False)\n",
    "\n",
    "max_lf = 0\n",
    "for index,rows in data.iterrows():\n",
    "    max_lf = max(max_lf,rows[1])\n",
    "\n",
    "print(max_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate triples\n",
    "data = pd.read_csv(\"<FILE>\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "triples = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0].lower()\n",
    "    p = rows[1].lower()\n",
    "    o = rows[2].lower()\n",
    "    \n",
    "    if [s,p,o] not in triples:\n",
    "        triples.append([s,p,o])\n",
    "\n",
    "print(len(triples))   \n",
    "df2 = pd.DataFrame(triples)\n",
    "df2.to_csv(\"<PATH>/originalremovingduplicates.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"<PATH>/sub_rel_obj_freq_score.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "remaining_rank_file = []\n",
    "remaining_or_trip_file = []\n",
    "\n",
    "rank_trip = []\n",
    "or_trip = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    f = rows[3]\n",
    "    score = rows[4]\n",
    "    if [s,p,o] in triples:\n",
    "        rank_trip.append([s,p,o,f,score])\n",
    "        or_trip.append([s,p,o])\n",
    "    else:\n",
    "        remaining_rank_file.append([s,p,o,f,score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(remaining_rank_file))\n",
    "df2 = pd.DataFrame(remaining_rank_file)\n",
    "df2.to_csv(\"<PATH>/remaining_ranking_triples.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3251ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in triples:\n",
    "    if item not in or_trip:\n",
    "        remaining_or_trip_file.append(item)\n",
    "    \n",
    "print(len(remaining_or_trip_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(or_trip))\n",
    "df2 = pd.DataFrame(remaining_or_trip_file)\n",
    "df2.to_csv(\"<PATH>/remaining_original_triples.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94272c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check weather the triples data contains the required concepts in the new dataset or not.\n",
    "\n",
    "data = pd.read_csv(\"<PATH>/rowi.csv\",header=None, sep=\",\", low_memory=False)\n",
    "termsList = []\n",
    "termsList1 = []\n",
    "for index,rows in data.iterrows():\n",
    "    terms = rows[0].lower()\n",
    "    present = rows[3].lower()\n",
    "    \n",
    "    if present == \"y\":\n",
    "        terms = replace_and_return_string(terms.lower())\n",
    "        termsList.append(terms)\n",
    "    elif present == \"n\":\n",
    "        terms = replace_and_return_string(terms.lower())\n",
    "        termsList1.append(terms)\n",
    "        \n",
    "        \n",
    "data = pd.read_csv(\"<DataFolder>/combined-removing-neg-relations.tsv\", header=None, sep=\"\\t\")\n",
    "count = count1 = count2 = count3 = 0\n",
    "presentList = []\n",
    "solist = []\n",
    "\n",
    "npresentList = []\n",
    "nsolist = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = replace_and_return_string(rows[0].lower())\n",
    "    p = replace_and_return_string(rows[1].lower())\n",
    "    o = replace_and_return_string(rows[2].lower())\n",
    "    \n",
    "    if s in termsList and o in termsList:\n",
    "        count +=1\n",
    "        presentList.append([s,p,o])\n",
    "    elif s in termsList or o in termsList:\n",
    "        count1 +=1\n",
    "        solist.append([s,p,o])\n",
    "        \n",
    "    if s in termsList1 and o in termsList1:\n",
    "        count2 +=1\n",
    "        npresentList.append([s,p,o])\n",
    "    elif s in termsList1 or o in termsList1:\n",
    "        count3 +=1\n",
    "        nsolist.append([s,p,o])\n",
    "          \n",
    "        \n",
    "print(count, count1, count2, count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(solist)\n",
    "df2.to_csv(\"<DataFolder>/init.csv\",index=False,header=None,sep=\"\\t\")\n",
    "\n",
    "df2 = pd.DataFrame(presentList)\n",
    "df2.to_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/withoutdup/subjectobject_CUIs.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9aac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = [\"Enterococci in water\",\n",
    "                    \"Fecal Coliform in water\",\n",
    "                    \"E.coli in marine water\",\n",
    "                    \"Fecal Indicator Bacteria in water\",\n",
    "                    \"E coli in water\",\n",
    "                    \"Water quality indicators\",\n",
    "                    \"Enteric Viruses\",\n",
    "                    \"Enteric infections from beaches\",\n",
    "                    \"Gastrointestinal Illness and beaches\",\n",
    "                    \"Beach water contamination\",\n",
    "                    \"Beach outbreaks\",\n",
    "                    \"Waterborne infections in divers\",\n",
    "                    \"Marine water quality\",\n",
    "                    \"Marine water point source pollution\",\n",
    "                    \"Quantitative microbial risk assessment and water\",\n",
    "                    \"Weather and waterborne illness\",\n",
    "                    \"Non cholera vibrio outbreaks\",\n",
    "                    \"Pfiesteria infection\",\n",
    "                    \"Red tides and illness\",\n",
    "                    \"Harmful algal blooms and illness\",\n",
    "                    \"Coastal water pollution\",\n",
    "                    \"Water salinity and illness\",\n",
    "                    \"Water characteristics and illness\",\n",
    "                    \"Tides and illness\",\n",
    "                    \"Harmful algal bloom\"]\n",
    "prev_keywords = []\n",
    "for k in key:\n",
    "    k = k.lower()\n",
    "    prev_keywords.append(k)\n",
    "    \n",
    "    if k in termsList:\n",
    "        print(k)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Keywords not in abstract list']  = pd.Series(termsList)\n",
    "df['Abstract Keywords List']  = pd.Series(prev_keywords)\n",
    "df['CUIS covered in SemMedDB']  = pd.Series(termsList)\n",
    "df['CUIS not covered in SemMedDB']  = pd.Series(termsList1)\n",
    "\n",
    "df.to_csv(\"<PATH>/info.tsv\",index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e814de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For extracting triples from keywords abstracts\n",
    "\n",
    "def extract_trip_from_abstract(filename):\n",
    "    dataset = []\n",
    "    garbagge = []\n",
    "    fulldata = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            if (\"|relation|\") in line:\n",
    "                texts = line.split(\"|\")\n",
    "                if len(texts)  > 10:\n",
    "                    s = texts[3]\n",
    "                    p = texts[8]\n",
    "                    o = texts[10]\n",
    "                    if [s,p,o] not in dataset:\n",
    "                        if s != \"\" and p != \"\" and o != \"\":\n",
    "                            dataset.append([s,p,o])\n",
    "                    fulldata.append([s,p,o])\n",
    "                else:\n",
    "                    garbagge.append(texts)\n",
    "                    \n",
    "    return fulldata, dataset, garbagge\n",
    "\n",
    "\n",
    "f, d, g = extract_trip_from_abstract(\"<PATH>/Abstract-related/fullkey.txt\")\n",
    "f1, d1,g1 = extract_trip_from_abstract(\"<PATH>/Abstract-related/nlmout.txt\")\n",
    "f2, d2,g2 = extract_trip_from_abstract(\"<PATH>/Abstract-related/algalout.txt\")\n",
    "\n",
    "dataset = d + d1 + d2\n",
    "fulldataset = f + f1 + f2\n",
    "garbagge = g + g1 + g2\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(\"<PATH>/Abstract-related/abstract-triples/withoutduplicates.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "   \n",
    "df1 = pd.DataFrame(garbagge)\n",
    "df1.to_csv(\"<PATH>/Abstract-related/abstract-triples/gar.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "\n",
    "df2 = pd.DataFrame(fulldataset)\n",
    "df2.to_csv(\"<PATH>/Abstract-related/abstract-triples/fulldata.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c112f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine abstract and semMeddb triples for stats\n",
    "\n",
    "data1 = pd.read_csv(\"<PATH>/Abstract-related/abs-sem-comb/withoutdup/abs.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "data2 = pd.read_csv(\"<PATH>/Abstract-related/abs-sem-comb/withoutdup/sem.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "combD = []\n",
    "for index,rows in data1.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    combD.append([s,p,o])\n",
    "    \n",
    "    \n",
    "count = 0\n",
    "for index,rows in data2.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    if [s,p,o] not in combD:\n",
    "        combD.append([s,p,o])\n",
    "    else:\n",
    "        count +=1\n",
    "        \n",
    "print(\"Duplicate triples in both files:\", count)\n",
    "print(len(data1), len(data2), len(combD))\n",
    "        \n",
    "df = pd.DataFrame(combD)\n",
    "df.to_csv(\"<PATH>/Abstract-related/abs-sem-comb/withoutdup/combined.tsv\",index=False,header=None,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "import os, glob\n",
    "\n",
    "url_list = set()\n",
    "rootdir = \"<PATH>/Abstract-related/backup\" +  \"/*\"\n",
    "\n",
    "top = 0 \n",
    "for filename in glob.iglob(rootdir, recursive=True):\n",
    "    if os.path.isfile(filename):\n",
    "#         print(filename)\n",
    "        count = 0 \n",
    "        pd = pandas.read_csv(filename, header = None)\n",
    "\n",
    "        for row in pd.itertuples():\n",
    "            key = row[2]\n",
    "            data = row[4]\n",
    "            url = row[3]\n",
    "            if data:\n",
    "                count +=1\n",
    "                if url not in url_list:\n",
    "                    url_list.add(url)\n",
    "        \n",
    "        top +=1\n",
    "        print(top, key,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b3995d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75628\n",
      "102287\n"
     ]
    }
   ],
   "source": [
    "#Extracting sentences corresponding to entities from Abstracts - 1 \n",
    "\n",
    "data = pd.read_csv(\"<PATH>/trip.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "final = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    final.append([s,p,o])\n",
    "    \n",
    "print(len(final))\n",
    "\n",
    "def extract_sentence_from_abstract(filename):\n",
    "    dataset = []\n",
    "    tripset = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        for index, line in enumerate(data):\n",
    "            if index < len(data) - 2:\n",
    "                \n",
    "                nextLine = data[index+2]\n",
    "                \n",
    "                if (\"|relation|\") in nextLine and (\"|relation|\") not in line:\n",
    "                    \n",
    "                    texts = nextLine.split(\"|\")\n",
    "                    textid = texts[0]\n",
    "                    \n",
    "                    sentence = line.split(textid)\n",
    "                    if len(sentence) > 1:\n",
    "                        sen = sentence[1].split(\"\\n\")[0]\n",
    "                        dataset.append(sen.lower())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "d2 = extract_sentence_from_abstract(\"<PATH>/Abstract-related/algalout.txt\")\n",
    "d = extract_sentence_from_abstract(\"<PATH>/Abstract-related/fullkey.txt\")\n",
    "d1 = extract_sentence_from_abstract(\"<PATH>/Abstract-related/nlmout.txt\")\n",
    "\n",
    "dataset = d + d1 + d2\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting sentences corresponding to entities from Abstracts -2 \n",
    "\n",
    "data = pd.read_csv(\"<PATH>/Abstract-related/rowi.csv\",header=None, sep=\",\", low_memory=False)\n",
    "termsList = []\n",
    "for index,rows in data.iterrows():\n",
    "    terms = rows[0].lower()\n",
    "    present = rows[3].lower()\n",
    "    \n",
    "    if present == \"y\":\n",
    "        terms = terms.lower()\n",
    "        termsList.append(terms)\n",
    "\n",
    "print(len(termsList))\n",
    "\n",
    "sen_present = []\n",
    "for item in dataset:\n",
    "    for terms in termsList:\n",
    "        if terms in item:\n",
    "            if item not in sen_present:\n",
    "                sen_present.append(item)\n",
    "            \n",
    "print(len(sen_present))\n",
    "\n",
    "df = pd.DataFrame(sen_present)\n",
    "df.to_csv(\"<PATH>/Abstract-related/abs-sentences/sentences-abs.txt\",index=False,header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
