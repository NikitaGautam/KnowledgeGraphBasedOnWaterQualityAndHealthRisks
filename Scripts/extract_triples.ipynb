{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdflib\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54aa1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import RDFS\n",
    "from urllib.parse import urldefrag\n",
    "from rdflib.term import BNode,Literal\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb896cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples(graph):\n",
    "    def get_frag(node):\n",
    "        if '#' in node:\n",
    "            return urldefrag(node)[1]\n",
    "        else:\n",
    "            return node\n",
    "    def get_english(node,graph):\n",
    "      if node in renameURIs:\n",
    "        return renameURIs[node]\n",
    "      elif (node,RDFS.label,None) in graph:\n",
    "        labelNode = [x[2] for x in graph.triples((node,RDFS.label,None))][0]\n",
    "        if isinstance(labelNode,Literal):\n",
    "          return labelNode.value\n",
    "        else:\n",
    "          return labelNode\n",
    "      elif (isinstance(node,Literal)):\n",
    "\n",
    "        return node.value\n",
    "      elif '#' in node.toPython():\n",
    "        return get_frag(node.toPython())\n",
    "\n",
    "\n",
    "    def change_case(string):\n",
    "        \n",
    "        if string is not None:\n",
    "            string = string.replace(\"-\", \"\")\n",
    "            string = string.replace(\" \", \"_\")\n",
    "            if string:\n",
    "                res = [string[0].lower()]\n",
    "                for c in string[1:]:\n",
    "                    if c in ('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "                        res.append('_')\n",
    "                        res.append(c.lower())\n",
    "                    else:\n",
    "                        res.append(c)\n",
    "\n",
    "                return ''.join(res)\n",
    "            return string\n",
    "\n",
    "\n",
    "    triples = []\n",
    "    for s, p, o in graph:\n",
    "      #out.write(\"\\t\".join([s,p,o]) +\"\\n\")\n",
    "      if isinstance(s,BNode) or isinstance(p,BNode) or isinstance(o,BNode) or  (p in (prefixURI,RDFS.label,contributorURI,licenseURI,commentURI,noteURI,definitionURI,deprecatedURI,seeAlsoURI,scopeNoteURI)) or (p == typeURI and o in (classURI,namedIndividualURI)) or (s == versionInfoURI or p == versionInfoURI or o ==versionInfoURI):\n",
    "        continue\n",
    "      s_name = change_case(get_english(s,graph))\n",
    "      p_name = change_case(get_english(p,graph))\n",
    "      o_name = change_case(get_english(o,graph))\n",
    "\n",
    "\n",
    "      if s_name is None or p_name is None or o_name is None:\n",
    "        continue\n",
    "      else:\n",
    "        triples.append([s_name,p_name,o_name])\n",
    "    return triples\n",
    "\n",
    "def init_graph(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        graph = rdflib.Graph()\n",
    "        graph.parse(f,format=\"ttl\")\n",
    "        \n",
    "        triples = get_triples(graph)\n",
    "    \n",
    "        return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24627675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"/Users/ngautam/Desktop/scrapper/onto/CRISPsubset.ttl\"\n",
    "filename1 = \"/Users/ngautam/Desktop/scrapper/onto/DOCEsubset.ttl\"\n",
    "filename2 = \"/Users/ngautam/Desktop/scrapper/onto/EPOsubset.ttl\"\n",
    "filename3 = \"/Users/ngautam/Desktop/scrapper/onto/IDOsubset.ttl\"\n",
    "filename4 = \"/Users/ngautam/Desktop/scrapper/onto/EXOsubset.ttl\"\n",
    "\n",
    "filename5 = \"/Users/ngautam/Desktop/scrapper/onto/ONR6.12update.ttl\"\n",
    "\n",
    "tri1 = init_graph(filename)\n",
    "tri2 = init_graph(filename1)\n",
    "tri3 = init_graph(filename2)\n",
    "tri4 = init_graph(filename3)\n",
    "tri5 = init_graph(filename4)\n",
    "tri6 = init_graph(filename5)\n",
    "\n",
    "tri = tri1 + tri2 + tri3 + tri4 + tri5\n",
    "triples = list(set(tuple(sorted(sub)) for sub in triples))\n",
    "\n",
    "df = pd.DataFrame(tri)\n",
    "df.to_csv(\"combined.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "df = pd.DataFrame(tri1)\n",
    "df.to_csv(\"crisp.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "df = pd.DataFrame(tri2)\n",
    "df.to_csv(\"doce.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "df = pd.DataFrame(tri3)\n",
    "df.to_csv(\"epo.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "df = pd.DataFrame(tri4)\n",
    "df.to_csv(\"ido.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "df = pd.DataFrame(tri5)\n",
    "df.to_csv(\"exo.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "df = pd.DataFrame(tri6)\n",
    "df.to_csv(\"final.tsv\",index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaf6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "graph = Graph()\n",
    "graph.parse('/Users/ngautam/Downloads/doce.ttl', format='ttl')\n",
    "# graph.parse('/Users/ngautam/Desktop/final.ttl', format='ttl')\n",
    "# graph.load('/Users/ngautam/Downloads/doce.ttl')\n",
    "\n",
    "s_list = []\n",
    "p_list = []\n",
    "o_list = []\n",
    "for s, p, o in graph:\n",
    "    s = s.split(\"/\")[-1]\n",
    "    s = s.split(\"#\")[-1]\n",
    "    s_list.append(s)\n",
    "    \n",
    "    p = p.split(\"/\")[-1]\n",
    "    p = p.split(\"#\")[-1]\n",
    "    p_list.append(p)\n",
    "    \n",
    "    o = o.split(\"/\")[-1]\n",
    "    o = o.split(\"#\")[-1]\n",
    "    o_list.append(o)\n",
    "\n",
    "print(len(s_list))\n",
    "\n",
    "\n",
    "new_s_list = []\n",
    "for index,item in enumerate(s_list):\n",
    "    new_s_list.append(change_case(item))\n",
    "\n",
    "new_p_list = []\n",
    "for index,item in enumerate(p_list):\n",
    "    new_p_list.append(change_case(item))\n",
    "\n",
    "\n",
    "new_o_list = []\n",
    "for index,item in enumerate(o_list):\n",
    "    new_o_list.append(change_case(item))\n",
    "\n",
    "\n",
    "with open(\"train.txt\", \"w\") as train:\n",
    "    for index, item in enumerate(new_s_list[0:640]):\n",
    "        line = new_s_list[index] + \"\\t\" + new_p_list[index] + \"\\t\" + new_o_list[index] + \"\\n\"\n",
    "        \n",
    "        train.write(line) \n",
    "\n",
    "with open(\"test.txt\", \"w\") as test:\n",
    "    index = 640 \n",
    "    for item in new_s_list[640:]:\n",
    "        line = new_s_list[index] + \"\\t\" + new_p_list[index] + \"\\t\" + new_o_list[index] + \"\\n\"\n",
    "        index +=1\n",
    "        \n",
    "        test.write(line)\n",
    "        \n",
    "        \n",
    "entities = set(new_s_list) \n",
    "for item in new_o_list:\n",
    "    entities.add(item)\n",
    "    \n",
    "print(len(entities))\n",
    "\n",
    "relations = set(new_p_list)\n",
    "print(len(relations))\n",
    "\n",
    "\n",
    "with open(\"entities.txt\", \"w\") as file:\n",
    "    for item in entities:\n",
    "        line = item +  \"\\n\"\n",
    "        \n",
    "        file.write(line) \n",
    "\n",
    "with open(\"relations.txt\", \"w\") as file:\n",
    "    for item in relations:\n",
    "        line = item +  \"\\n\"\n",
    "        \n",
    "        file.write(line) \n",
    "\n",
    "new_s_list1 = []\n",
    "for index,item in enumerate(s_list):\n",
    "    item = item.replace(\"-\", \" \")\n",
    "    new_s_list1.append(item)\n",
    "\n",
    "new_p_list1 = []\n",
    "for index,item in enumerate(p_list):\n",
    "    item = item.replace(\"-\", \" \")\n",
    "    new_p_list1.append(item)\n",
    "\n",
    "\n",
    "new_o_list1 = []\n",
    "for index,item in enumerate(o_list):\n",
    "    item = item.replace(\"-\", \" \")\n",
    "    new_o_list1.append(item)\n",
    "    \n",
    "\n",
    "entities = set(new_s_list1) \n",
    "for item in new_o_list1:\n",
    "    entities.add(item)\n",
    "\n",
    "relations = set(new_p_list1)\n",
    "    \n",
    "    \n",
    "with open(\"entitiestotext.txt\", \"w\") as file:\n",
    "    for item in entities:\n",
    "        line = item +  \"\\n\"\n",
    "        \n",
    "        file.write(line) \n",
    "\n",
    "with open(\"relationstotext.txt\", \"w\") as file:\n",
    "    for item in relations:\n",
    "        line = item +  \"\\n\"\n",
    "        \n",
    "        file.write(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf786ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = []\n",
    "def extract_rel_instances(data):\n",
    "    \n",
    "    p_list = set()\n",
    "    e_list = set()\n",
    "\n",
    "    for index,rows in data.iterrows():\n",
    "        try:\n",
    "            s_name = rows[0].replace(\" \", \"_\")\n",
    "            p_name = rows[1].replace(\" \", \"_\")\n",
    "            o_name = rows[2].replace(\" \", \"_\")\n",
    "            p_list.add(p_name)\n",
    "            e_list.add(s_name)\n",
    "            e_list.add(o_name)\n",
    "\n",
    "\n",
    "            triples.append([s_name,p_name,o_name])\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "\n",
    "    return p_list, e_list, triples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dad11f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_relations_instances(p_list,e_list, triples):\n",
    "    count_list = []\n",
    "    for predicate in p_list:\n",
    "        count = 0\n",
    "        for item in triples:\n",
    "            if item[1] == predicate:\n",
    "                count +=1 \n",
    "        count_list.append(count)\n",
    "\n",
    "    count_dict = []\n",
    "    p_list = list(p_list)\n",
    "\n",
    "    for index,item in enumerate(count_list):\n",
    "        count_dict.append([p_list[index], item])\n",
    "\n",
    "    print(len(count_dict))  \n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(count_dict, columns=[\"Relationship\", \"Number of instances\"])\n",
    "    return df,count_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17af4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relations(count_dict, triples, threshold):\n",
    "    new_rels = set()\n",
    "    for key, value in count_dict:\n",
    "        if value > threshold:\n",
    "            new_rels.add(key)\n",
    "\n",
    "    print(new_rels)\n",
    "    newtriples = []\n",
    "    new_ent = set()\n",
    "    for s,p,o in triples:\n",
    "        if p in new_rels:\n",
    "            newtriples.append([s,p,o])\n",
    "            new_ent.add(s)\n",
    "            new_ent.add(o)\n",
    "    return newtriples, new_ent, new_rels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03288b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, data):\n",
    "    with open (filename,\"w\") as file:\n",
    "        for item in data:\n",
    "            file.write(item)\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "def save_data_text(filename, data):\n",
    "    with open (filename,\"w\") as file:\n",
    "        for item in data:\n",
    "            text = item + \"\\t\" + item.replace(\"_\", \" \")\n",
    "            file.write(text)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cffdd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6778b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate negative examples: doce + sweet\n",
    "import random\n",
    "\n",
    "def extract_entities(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        entity = []\n",
    "        data = f.readlines()\n",
    "        for d in data:\n",
    "            d=d.split(\"\\n\")[0]\n",
    "            entity.append(d)\n",
    "            \n",
    "    return entity\n",
    "\n",
    "def current_triples(data):\n",
    "    curr = []\n",
    "    for index,rows in data.iterrows():\n",
    "        s = rows[0]\n",
    "        p = rows[1]\n",
    "        o = rows[2]\n",
    "        curr.append([s,p,o])\n",
    "    return curr\n",
    "    \n",
    "\n",
    "def create_neg_examples(data, entities):\n",
    "    triples = []\n",
    "    curr = current_triples(data)\n",
    "    for index,rows in data.iterrows():\n",
    "        s = rows[0]\n",
    "        p = rows[1]\n",
    "        o = rows[2]\n",
    "        \n",
    "        checkpoint = True\n",
    "        while checkpoint:\n",
    "            neg_o = random.choice(entities)\n",
    "            if [s,p,neg_o] not in curr and [neg_o, p, o] not in curr:\n",
    "                triples.append([s,p,o, 1])\n",
    "                if random.choice(range(0,20000)) % 2 == 0:\n",
    "                    triples.append([s,p,neg_o, -1])\n",
    "                else:\n",
    "                    triples.append([neg_o,p,o, -1])\n",
    "                checkpoint = False\n",
    "            \n",
    "    return triples\n",
    "\n",
    "\n",
    "\n",
    "def create_neg_eg_and_save(source, destination, entsource):\n",
    "    entities = extract_entities(entsource)\n",
    "    data = pd.read_csv(source, sep=\"\\t\",header=None)\n",
    "    triples = create_neg_examples(data, entities)\n",
    "    df = pd.DataFrame(triples)\n",
    "    df.to_csv(destination,index=False,header=False,sep=\"\\t\")\n",
    "\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "21f6408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378135\n",
      "47\n",
      "             Relationship  Number of instances\n",
      "0         associated_with                 5500\n",
      "1   associated_with_spec_                    6\n",
      "2                measures                  148\n",
      "3        process_of_spec_                   18\n",
      "4            treats_spec_                   11\n",
      "5             predisposes                 1675\n",
      "6       location_of_spec_                   34\n",
      "7                 same_as                    3\n",
      "8          precedes_spec_                    1\n",
      "9              process_of                 5124\n",
      "10          treats_infer_                    8\n",
      "11                   uses                   48\n",
      "12             lower_than                    1\n",
      "13  interacts_with_infer_                    1\n",
      "14               precedes                  469\n",
      "15        administered_to                   37\n",
      "16         augments_spec_                    1\n",
      "17       manifestation_of                  261\n",
      "18          coexists_with                 4587\n",
      "19         produces_spec_                    9\n",
      "20  administered_to_spec_                    1\n",
      "21                 causes                 8952\n",
      "22            complicates                  172\n",
      "23          part_of_spec_                   17\n",
      "24         interacts_with                 7419\n",
      "25              occurs_in                  402\n",
      "26            location_of                12839\n",
      "27                affects                 4286\n",
      "28               augments                  595\n",
      "29    coexists_with_spec_                   10\n",
      "30               produces                 2137\n",
      "31         measurement_of                    6\n",
      "32              method_of                    1\n",
      "33          compared_with                   65\n",
      "34          affects_spec_                   12\n",
      "35              diagnoses                  924\n",
      "36                    isa                 1683\n",
      "37             stimulates                  225\n",
      "38                part_of                 5702\n",
      "39                 treats                 9394\n",
      "40           causes_spec_                   35\n",
      "41               prevents                 1669\n",
      "42               disrupts                  776\n",
      "43   interacts_with_spec_                    9\n",
      "44            converts_to                   13\n",
      "45            higher_than                   21\n",
      "46               inhibits                  321\n"
     ]
    }
   ],
   "source": [
    "#Extract Triples\n",
    "\n",
    "from langdetect import DetectorFactory, detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def get_triples_umls(filename, req_list):\n",
    "    triples = []\n",
    "    data = pd.read_csv(filename, sep=\",\")\n",
    "    for index,rows in data.iterrows():\n",
    "        one = str(rows[0])\n",
    "        two = str(rows[1])\n",
    "        three = str(rows[2])\n",
    "\n",
    "        try:\n",
    "            if detect(one) == \"en\" and detect(two) == \"en\" and detect(three) == \"en\":\n",
    "                s_name = one.lower().replace(\" \", \"_\")\n",
    "                p_name = two.lower().replace(\" \", \"_\")\n",
    "                o_name = three.lower().replace(\" \", \"_\")\n",
    "                triples.append([s_name,p_name,o_name])\n",
    "\n",
    "        except LangDetectException:\n",
    "            continue\n",
    "    return triples\n",
    "\n",
    "import re\n",
    "\n",
    "def replace_and_return_string(string):\n",
    "    replacing_strings=[\" \", \"__\"]\n",
    "    string = re.sub(r\"^\\s+|\\s+$\", \"\", string)\n",
    "    string = re.sub('[^a-zA-Z0-9 \\n\\.]', '_', string)\n",
    "    string = string.replace(\".\",\"\")\n",
    "    for chars in replacing_strings:\n",
    "        string = string.replace(chars, \"_\")\n",
    "    \n",
    "    return string\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_triples_umls1(filename, req_list):\n",
    "    triples = []\n",
    "    data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    for index,rows in data.iterrows():\n",
    "        one = str(rows[0])\n",
    "        two = str(rows[1])\n",
    "        three = str(rows[2])\n",
    "        s_name = replace_and_return_string(one.lower())\n",
    "        p_name = replace_and_return_string(two.lower())\n",
    "        o_name = replace_and_return_string(three.lower())\n",
    "        triples.append([s_name,p_name,o_name])\n",
    "    return triples\n",
    "\n",
    "# triples1 = get_triples_umls(\"/Users/ngautam/Desktop/david1trip.csv\", [])\n",
    "triples1 = get_triples_umls1(\"/Users/ngautam/Desktop/finalData/init.tsv\", [])\n",
    "\n",
    "df3 = pd.DataFrame(triples1)\n",
    "df3.to_csv(\"/Users/ngautam/Desktop/finalData/david1trip.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "data1 = pd.read_csv(\"~/Desktop/finalData/david1trip.tsv\", sep=\"\\t\")\n",
    "\n",
    "p_list, e_list, triples = extract_rel_instances(data1)\n",
    "new_ent = e_list\n",
    "new_rels = p_list\n",
    "\n",
    "print(len(triples))\n",
    "df,count_dict = show_relations_instances(p_list, e_list, triples1)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ba1976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"/Users/ngautam/Desktop/finalData/entities.txt\", new_ent)\n",
    "save_data(\"/Users/ngautam/Desktop/finalData/relations.txt\", new_rels)\n",
    "\n",
    "save_data_text(\"/Users/ngautam/Desktop/finalData/entity2text.txt\", new_ent)\n",
    "save_data_text(\"/Users/ngautam/Desktop/finalData/relation2text.txt\", new_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Relation Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de2426b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_relation_instances(p_list,e_list, triples):\n",
    "    reldict = {}\n",
    "    for predicate in p_list:\n",
    "        itemslist = []\n",
    "        \n",
    "        for item in triples:\n",
    "            if item[1] == predicate:\n",
    "                itemslist.append(item)  \n",
    "                \n",
    "        reldict[predicate] = itemslist\n",
    "    return reldict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Relation prediction \n",
    "\n",
    "relations = combine_relation_instances(p_list, e_list, triples1)\n",
    "\n",
    "count =  0\n",
    "for key,value in relations.items():\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(value)\n",
    "    \n",
    "    train, validate, test= train_validate_test_split(df, train_percent=.8, validate_percent=.1)\n",
    "    train.to_csv(\"/Users/ngautam/Desktop/finalData/for-rel/train.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "    validate.to_csv(\"/Users/ngautam/Desktop/finalData/for-rel/dev.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "    test.to_csv(\"/Users/ngautam/Desktop/finalData/for-rel/test.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "    count += len(value)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/for-rel/train.tsv\") \n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/for-rel/test.tsv\") \n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/for-rel/dev.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "18eb3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required for LINK and RELATION\n",
    "\n",
    "\n",
    "def shuffle_data_and_save(file):\n",
    "    data = pd.read_csv(file, sep=\"\\t\")\n",
    "    df = data.sample(frac=1)\n",
    "    df.to_csv(file,index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "84dd4ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'associated_with', 'measures', 'predisposes', 'process_of', 'precedes', 'manifestation_of', 'coexists_with', 'causes', 'complicates', 'interacts_with', 'occurs_in', 'location_of', 'affects', 'augments', 'produces', 'diagnoses', 'isa', 'stimulates', 'part_of', 'treats', 'prevents', 'disrupts', 'inhibits'}\n",
      "23\n",
      "23\n",
      "        Relationship  Number of instances\n",
      "0    associated_with                 5500\n",
      "1           measures                  148\n",
      "2        predisposes                 1675\n",
      "3         process_of                 5124\n",
      "4           precedes                  469\n",
      "5   manifestation_of                  261\n",
      "6      coexists_with                 4587\n",
      "7             causes                 8952\n",
      "8        complicates                  172\n",
      "9     interacts_with                 7419\n",
      "10         occurs_in                  402\n",
      "11       location_of                12839\n",
      "12           affects                 4286\n",
      "13          augments                  595\n",
      "14          produces                 2137\n",
      "15         diagnoses                  924\n",
      "16               isa                 1683\n",
      "17        stimulates                  225\n",
      "18           part_of                 5702\n",
      "19            treats                 9394\n",
      "20          prevents                 1669\n",
      "21          disrupts                  776\n",
      "22          inhibits                  321\n"
     ]
    }
   ],
   "source": [
    "#for Filtering relations - Part 1 \n",
    "#STEP 1\n",
    "\n",
    "new_triples, new_ent, new_rels = filter_relations(count_dict, triples1, 100)\n",
    "print(len(new_rels))\n",
    "df,count_dict = show_relations_instances(new_rels, new_ent, new_triples)\n",
    "print(df)\n",
    "\n",
    "\n",
    "df_umls = pd.DataFrame(new_triples)\n",
    "df_umls.to_csv(\"~/Desktop/finalData/filtered/final.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "save_data(\"/Users/ngautam/Desktop/finalData/filtered/entities.txt\", new_ent)\n",
    "save_data(\"/Users/ngautam/Desktop/finalData/filtered/relations.txt\", new_rels)\n",
    "\n",
    "save_data_text(\"/Users/ngautam/Desktop/finalData/filtered/entity2text.txt\", new_ent)\n",
    "save_data_text(\"/Users/ngautam/Desktop/finalData/filtered/relation2text.txt\", new_rels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d602b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2\n",
    "#For Relation - Viewing subject| object - with each unique relation\n",
    "\n",
    "data = pd.read_csv(\"~/Desktop/finalData/filtered/final.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "p_list = set()\n",
    "e_list = set()\n",
    "dictKey = defaultdict(list)\n",
    "\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0].lower()\n",
    "    p = rows[1].lower()\n",
    "    o = rows[2].lower()\n",
    "    \n",
    "    p_list.add(p)\n",
    "    e_list.add(s)\n",
    "    e_list.add(o)\n",
    "    key = s+ \"|\" + o\n",
    "    dictKey[key].append(p)\n",
    "        \n",
    "\n",
    "trip1 = []\n",
    "maxV = 0\n",
    "for key, value in dictKey.items():\n",
    "    maxV = max(maxV,len(value))\n",
    "    trip1.append([key, value, len(value)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b306b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(trip1)\n",
    "df2.to_csv(\"~/Desktop/finalData/filtered/so-r.tsv\",index=False,header=[\"subject | object\", \"relations\", \"count\"],sep=\"\\t\")\n",
    "print(maxV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "69e8a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 21339\n",
      "53921 21339\n"
     ]
    }
   ],
   "source": [
    "#STEP 3\n",
    "out_trip = defaultdict(list)\n",
    "overall = []\n",
    "count = 0 \n",
    "for rows in trip1:\n",
    "    if rows[2] > 1:\n",
    "        s, o = rows[0].split(\"|\")\n",
    "        for p in rows[1]:\n",
    "            trip = [s,p,o]\n",
    "            if trip in new_triples: \n",
    "                count +=1\n",
    "                overall.append(trip)\n",
    "                out_trip[rows[0]].append(trip)\n",
    "                new_triples.remove(trip)\n",
    "\n",
    "print(\"Done\", count)\n",
    "print(len(new_triples), len(overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c60ad652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16804 2311 2224\n",
      "21339\n"
     ]
    }
   ],
   "source": [
    "#STEP 4\n",
    "\n",
    "out_list = []\n",
    "for key, value in out_trip.items():\n",
    "    out_list.append(value)\n",
    "    \n",
    "n = len(out_list)\n",
    "id1 = int(n * 0.8)\n",
    "id2 = int(n * 0.8 * 0.1) + int(n * 0.8)\n",
    "\n",
    "train3 = []\n",
    "test3 = []\n",
    "validate3 = []\n",
    "count = 0\n",
    "\n",
    "for item in out_list:\n",
    "    if count >=0 and count < id1:\n",
    "        for val in item:\n",
    "            train3.append(val)\n",
    "    elif count >= id1 and count < id2:\n",
    "        for val in item:\n",
    "            test3.append(val)\n",
    "    elif count >=id2 and count < n:\n",
    "        for val in item:\n",
    "            validate3.append(val)\n",
    "    count +=1\n",
    "\n",
    "\n",
    "\n",
    "print(len(train3), len(test3), len(validate3))\n",
    "print(len(train3) + len(test3) + len(validate3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "83d5f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60424 7033 7803\n"
     ]
    }
   ],
   "source": [
    "#Continue for relation filtering  - Part2\n",
    "#STEP 5\n",
    "\n",
    "# train3, validate3, test3 = train_validate_test_split(df_umls, train_percent=.6, validate_percent=.2)\n",
    "\n",
    "relations = combine_relation_instances(new_rels, new_ent, new_triples)\n",
    "count =  0\n",
    "\n",
    "\n",
    "train3 = pd.DataFrame(train3)\n",
    "validate3 = pd.DataFrame(validate3)\n",
    "test3 = pd.DataFrame(test3)\n",
    "\n",
    "for key,value in relations.items():\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(value)\n",
    "    \n",
    "    tr, v, te= train_validate_test_split(df, train_percent=.8, validate_percent=.1)\n",
    "    count += len(value)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train3 = pd.concat([train3, tr])\n",
    "    validate3 = pd.concat([validate3, v])\n",
    "    test3 = pd.concat([test3, te])\n",
    "\n",
    "train3.to_csv(\"/Users/ngautam/Desktop/finalData/filtered/train.tsv\",index=False,header=False,sep=\"\\t\", mode='a')\n",
    "validate3.to_csv(\"/Users/ngautam/Desktop/finalData/filtered/dev.tsv\",index=False,header=False,sep=\"\\t\", mode='a')\n",
    "test3.to_csv(\"/Users/ngautam/Desktop/finalData/filtered/test.tsv\",index=False,header=False,sep=\"\\t\", mode='a')\n",
    "\n",
    "print(len(train3), len(test3), len(validate3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5c891893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/filtered/train.tsv\") \n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/filtered/test.tsv\") \n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/filtered/dev.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "97e101a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Link Prediction STEP 1\n",
    "\n",
    "def view_link_relation_instances(p_list,e_list, triples):\n",
    "    reldict = {}\n",
    "    for item in triples:\n",
    "        reldict[item[0]+\"|\"+item[1]] = []\n",
    "    for item in triples:\n",
    "        reldict[item[0]+\"|\"+item[1]].append(item[2])\n",
    "    \n",
    "    return reldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "318c3678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#STEP 2\n",
    "reldict = view_link_relation_instances(p_list, e_list, triples1)\n",
    "count_dict = []\n",
    "\n",
    "for key, value in reldict.items():\n",
    "    count_dict.append([key, value, len(value)]) \n",
    "\n",
    "df2 = pd.DataFrame(count_dict)\n",
    "df2.to_csv(\"~/Desktop/finalData/for-link/er.tsv\",index=False,header=[\"subject | relation\", \"tail entity\", \"number of instances\"],sep=\"\\t\")\n",
    "print(\"done\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "7996d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 51046\n",
      "24582 51046\n"
     ]
    }
   ],
   "source": [
    "#STEP 3\n",
    "\n",
    "new_trip = triples1.copy()\n",
    " \n",
    "out_trip = defaultdict(list)\n",
    "overall = []\n",
    "count = 0 \n",
    "\n",
    "for rows in count_dict:\n",
    "    if rows[2] > 1:\n",
    "        s, p = rows[0].split(\"|\")\n",
    "        for o in rows[1]:\n",
    "            trip = [s,p,o]\n",
    "            if trip in new_trip: \n",
    "                count +=1\n",
    "                overall.append(trip)\n",
    "                out_trip[rows[0]].append(trip)\n",
    "                new_trip.remove(trip)\n",
    "\n",
    "print(\"Done\", count)\n",
    "print(len(new_trip), len(overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "07b8791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47748 1247 2051\n",
      "51046\n"
     ]
    }
   ],
   "source": [
    "#STEP 4\n",
    "\n",
    "out_list = []\n",
    "for key, value in out_trip.items():\n",
    "    out_list.append(value)\n",
    "    \n",
    "n = len(out_list)\n",
    "id1 = int(n * 0.8)\n",
    "id2 = int(n * 0.8 * 0.1) + int(n * 0.8)\n",
    "\n",
    "train2 = []\n",
    "test2 = []\n",
    "validate2 = []\n",
    "count = 0\n",
    "\n",
    "for item in out_list:\n",
    "    if count >=0 and count < id1:\n",
    "        for val in item:\n",
    "            train2.append(val)\n",
    "    elif count >= id1 and count < id2:\n",
    "        for val in item:\n",
    "            test2.append(val)\n",
    "    elif count >=id2 and count < n:\n",
    "        for val in item:\n",
    "            validate2.append(val)\n",
    "    count +=1\n",
    "\n",
    "\n",
    "\n",
    "print(len(train2), len(test2), len(validate2))\n",
    "print(len(train2) + len(test2) + len(validate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "96343002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_link_relation_instances(p_list, e_list, triples):\n",
    "    linkdict = {}\n",
    "    for item in triples:\n",
    "        linkdict[item[0]+\"|\"+item[1]] = []\n",
    "    for item in triples:\n",
    "        linkdict[item[0]+\"|\"+item[1]].append(item)\n",
    "    return linkdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "55c9b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64955 4935 5738\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#STEP 5\n",
    "\n",
    "# data = combine_link_relation_instances(p_list, e_list, new_trip)\n",
    "# count =  0\n",
    "\n",
    "\n",
    "train2 = pd.DataFrame(train2)\n",
    "validate2 = pd.DataFrame(validate2)\n",
    "test2 = pd.DataFrame(test2)\n",
    "\n",
    "# for key,value in data.items():\n",
    "#     pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.DataFrame(new_trip)\n",
    "\n",
    "tr, v, te= train_validate_test_split(df, train_percent=.7, validate_percent=.15)\n",
    "count += len(value)\n",
    "\n",
    "\n",
    "train2 = pd.concat([train2, tr])\n",
    "validate2 = pd.concat([validate2, v])\n",
    "test2 = pd.concat([test2, te])\n",
    "\n",
    "train2.to_csv(\"/Users/ngautam/Desktop/finalData/for-link/train.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "validate2.to_csv(\"/Users/ngautam/Desktop/finalData/for-link/dev.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "test2.to_csv(\"/Users/ngautam/Desktop/finalData/for-link/test.tsv\",index=False,header=False,sep=\"\\t\", mode=\"a\")\n",
    "\n",
    "\n",
    "print(len(train2), len(test2), len(validate2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a3eaff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/for-link/train.tsv\") \n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/for-link/test.tsv\") \n",
    "shuffle_data_and_save(\"/Users/ngautam/Desktop/finalData/for-link/dev.tsv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08799968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7542a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Triple classification \n",
    "\n",
    "train3, validate3, test3 = train_validate_test_split(df3, train_percent=.8, validate_percent=.1)\n",
    "            \n",
    "train3.to_csv(\"/Users/ngautam/Desktop/finalData/for-triple/train.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "validate3.to_csv(\"/Users/ngautam/Desktop/finalData/for-triple/dev1.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "test3.to_csv(\"/Users/ngautam/Desktop/finalData/for-triple/test1.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "\n",
    "create_neg_eg_and_save(\"/Users/ngautam/Desktop/finalData/for-triple/test1.tsv\", \n",
    "                       \"/Users/ngautam/Desktop/finalData/for-triple/test.tsv\",\n",
    "                      \"/Users/ngautam/Desktop/finalData/for-triple/entities.txt\")\n",
    "\n",
    "create_neg_eg_and_save(\"/Users/ngautam/Desktop/finalData/for-triple/dev1.tsv\", \n",
    "                       \"/Users/ngautam/Desktop/finalData/for-triple/dev.tsv\",\n",
    "                      \"/Users/ngautam/Desktop/finalData/for-triple/entities.txt\")\n",
    "    \n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519401d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected instances > 100 - Old Implementation\n",
    "\n",
    "# rel_required = [\"has_operand\",\"has_numeric_value\" ,\"same_as\",\"range\", \"sub_property_of\", \"temporal_part_of\", \"has_end_time\", \"has_start_time\", \"equivalent_class\", \"sub_class_of\", \"type_of\"]\n",
    "# rel_required_doce = [\"has_applicable_unit\", \"sub_class_of\", \"type_of\"]\n",
    "\n",
    "def get_triples(filename, req_list):\n",
    "    triples = []\n",
    "    data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    for index,rows in data.iterrows():\n",
    "        s_name = rows[0].replace(\" \", \"_\")\n",
    "        p_name = rows[1].replace(\" \", \"_\")\n",
    "        o_name = rows[2].replace(\" \", \"_\")\n",
    "#         if p_name in req_list:\n",
    "        triples.append([s_name,p_name,o_name])\n",
    "            \n",
    "    return triples\n",
    "\n",
    "\n",
    "triples1 = get_triples(\"/Users/ngautam/Desktop/scrapper/doce.txt\", [])\n",
    "triples2 = get_triples(\"/Users/ngautam/Desktop/scrapper/sweet.txt\", [])\n",
    "\n",
    "# triples = triples1 + triples2\n",
    "\n",
    "df1 = pd.DataFrame(triples1)\n",
    "df1.to_csv(\"final1.txt\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(triples2)\n",
    "df2.to_csv(\"final2.txt\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train1, validate1, test1 = train_validate_test_split(df1)\n",
    "train2, validate2, test2 = train_validate_test_split(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.to_csv(\"/Users/ngautam/Desktop/scrapper/doce1/train3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "validate1.to_csv(\"/Users/ngautam/Desktop/scrapper/doce1/dev3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "test1.to_csv(\"/Users/ngautam/Desktop/scrapper/doce1/test3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "\n",
    "train2.to_csv(\"/Users/ngautam/Desktop/scrapper/sweet1/train3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "validate2.to_csv(\"/Users/ngautam/Desktop/scrapper/sweet1/dev3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "test2.to_csv(\"/Users/ngautam/Desktop/scrapper/sweet1/test3.tsv\",index=False,header=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4cfa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For UMLS - from paper\n",
    "train1, validate1, test1 = train_validate_test_split(df_umls)\n",
    "\n",
    "train1.to_csv(\"/Users/ngautam/Desktop/scrapper/umls1/train3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "validate1.to_csv(\"/Users/ngautam/Desktop/scrapper/umls1/dev3.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "test1.to_csv(\"/Users/ngautam/Desktop/scrapper/umls1/test3.tsv\",index=False,header=False,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804634d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate negative examples: - Older\n",
    "\n",
    "def create_neg_examples(data):\n",
    "    triples = []\n",
    "    init = \"\"\n",
    "    for index,rows in data.iterrows():\n",
    "        s = rows[0]\n",
    "        p = rows[1]\n",
    "        o = rows[2]\n",
    "        \n",
    "        try:\n",
    "            neg_o = data.iloc[index+1][2]\n",
    "        except Exception as ex:\n",
    "            neg_o = init\n",
    "        init = o\n",
    "        \n",
    "        triples.append([s,p,o, 1])\n",
    "        triples.append([s,p,neg_o, -1])\n",
    "    return triples\n",
    "\n",
    "    \n",
    "data = pd.read_csv(\"/Users/ngautam/Desktop/scrapper/umls1/test3.tsv\", sep=\"\\t\",header=None)\n",
    "data1 = pd.read_csv(\"/Users/ngautam/Desktop/scrapper/umls1/dev3.tsv\", sep=\"\\t\",header=None)\n",
    "\n",
    "triples1 = create_neg_examples(data)\n",
    "df1 = pd.DataFrame(triples1)\n",
    "df1.to_csv(\"/Users/ngautam/Desktop/scrapper/umls1/test.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "triples2 = create_neg_examples(data1)\n",
    "df2 = pd.DataFrame(triples2)\n",
    "df2.to_csv(\"/Users/ngautam/Desktop/scrapper/umls1/dev.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final UMLS Dataset Cleaning - Removing neg relations, inverse relations and same subject-object \n",
    "\n",
    "from collections import defaultdict\n",
    "data = pd.read_csv(\"/Users/ngautam/Desktop/finalData/combined.tsv\",header=None, low_memory=False, sep=\"\\t\")\n",
    "\n",
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0 \n",
    "p_list = set()\n",
    "e_list = set()\n",
    "triples = []\n",
    "dictKey = defaultdict(list)\n",
    "\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0].lower()\n",
    "    p = rows[1].lower()\n",
    "    o = rows[2].lower()\n",
    "    if s == o: \n",
    "        count +=1\n",
    "    elif [o,p,s] in triples:\n",
    "        count1 +=1\n",
    "    elif p.startswith(\"neg_\"):\n",
    "        count2 +=1\n",
    "#     elif [s,p,o] not in triples:\n",
    "    else:\n",
    "        triples.append([s,p,o])\n",
    "        p_list.add(p)\n",
    "        e_list.add(s)\n",
    "        e_list.add(o)\n",
    "        \n",
    "        \n",
    "        key = s+ \"|\" + o\n",
    "        dictKey[key].append(p)\n",
    "        \n",
    "        \n",
    "        \n",
    "print(count, count1,count2, len(triples))\n",
    "  \n",
    "df,count_dict = show_relations_instances(p_list, e_list, triples)\n",
    "print(df)  \n",
    "\n",
    "df1 = pd.DataFrame(triples)\n",
    "df1.to_csv(\"/Users/ngautam/Desktop/finalData/init.tsv\",index=False,header=False,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Ranking tool\n",
    "\n",
    "data = pd.read_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/ranking/david1trip.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "data.to_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/ranking/sub_rel_obj.txt\",header=None, sep=\" \", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating frequency score/ranking for subject object \n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('./stats.txt', sep='<>| ', header=None, skiprows=1,\n",
    "                 names=['sub', 'rel', 'obj', 'rank', 'stat', 'f012',\n",
    "                        'f0', 'f1', 'f2', 'f01', 'f02', 'f12'], engine='python')\n",
    "deg = pd.read_csv('./concept_degree.txt', sep=' ', header=None, names=['cui', 'deg'])\n",
    "cui2deg = dict(zip(deg['cui'], deg['deg']))\n",
    "\n",
    "def lookup(cui):\n",
    "    try:\n",
    "        return cui2deg[cui]\n",
    "    except Exception as ex:\n",
    "        return 0\n",
    "\n",
    "df['sub_deg'] = df.apply(lambda row: lookup(row['sub']), axis=1)\n",
    "df['obj_deg'] = df.apply(lambda row: lookup(row['obj']), axis=1)\n",
    "\n",
    "\n",
    "names = ['stat', 'sub_deg', 'obj_deg']\n",
    "x = df[names].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_norm = min_max_scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_norm, columns=names, index = df.index)\n",
    "df[['stat_norm', 'sub_deg_norm', 'obj_deg_norm']] = df_temp\n",
    "df['score'] = df['stat_norm'] + df['sub_deg_norm'] + df['obj_deg_norm']\n",
    "\n",
    "\n",
    "df.to_csv('./sub_rel_obj_freq_score.tsv', sep='\\t', header=None, columns=['sub', 'rel', 'obj', 'f012', 'score'],\n",
    "          index=False, float_format='%.15f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Desktop/finalData/test/concept_degree.txt\",header=None, sep=\" \", low_memory=False)\n",
    "\n",
    "max_lf = 0\n",
    "for index,rows in data.iterrows():\n",
    "    max_lf = max(max_lf,rows[1])\n",
    "\n",
    "max_lf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Desktop/finalData/david1trip.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "triples = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0].lower()\n",
    "    p = rows[1].lower()\n",
    "    o = rows[2].lower()\n",
    "    \n",
    "    if [s,p,o] not in triples:\n",
    "        triples.append([s,p,o])\n",
    "\n",
    "print(len(triples))   \n",
    "df2 = pd.DataFrame(triples)\n",
    "df2.to_csv(\"~/Desktop/finalData/originalremovingduplicates.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Desktop/finalData/test/sub_rel_obj_freq_score.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "remaining_rank_file = []\n",
    "remaining_or_trip_file = []\n",
    "\n",
    "rank_trip = []\n",
    "or_trip = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    f = rows[3]\n",
    "    score = rows[4]\n",
    "    if [s,p,o] in triples:\n",
    "        rank_trip.append([s,p,o,f,score])\n",
    "        or_trip.append([s,p,o])\n",
    "    else:\n",
    "        remaining_rank_file.append([s,p,o,f,score])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(remaining_rank_file))\n",
    "df2 = pd.DataFrame(remaining_rank_file)\n",
    "df2.to_csv(\"~/Desktop/finalData/remaining_ranking_triples.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3251ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in triples:\n",
    "    if item not in or_trip:\n",
    "        remaining_or_trip_file.append(item)\n",
    "    \n",
    "print(len(remaining_or_trip_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(or_trip))\n",
    "df2 = pd.DataFrame(remaining_or_trip_file)\n",
    "df2.to_csv(\"~/Desktop/finalData/remaining_original_triples.tsv\",index=False,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94272c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check weather the Y/N terms are present in the dataset or not\n",
    "\n",
    "data = pd.read_csv(\"~/Desktop/Abstract-related/rowi.csv\",header=None, sep=\",\", low_memory=False)\n",
    "termsList = []\n",
    "termsList1 = []\n",
    "for index,rows in data.iterrows():\n",
    "    terms = rows[0].lower()\n",
    "    present = rows[3].lower()\n",
    "    \n",
    "    if present == \"y\":\n",
    "        terms = replace_and_return_string(terms.lower())\n",
    "        termsList.append(terms)\n",
    "    elif present == \"n\":\n",
    "        terms = replace_and_return_string(terms.lower())\n",
    "        termsList1.append(terms)\n",
    "        \n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/Users/ngautam/Desktop/finalData/combined-removing-neg-relations.tsv\", header=None, sep=\"\\t\")\n",
    "count = count1 = count2 = count3 = 0\n",
    "presentList = []\n",
    "solist = []\n",
    "\n",
    "npresentList = []\n",
    "nsolist = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = replace_and_return_string(rows[0].lower())\n",
    "    p = replace_and_return_string(rows[1].lower())\n",
    "    o = replace_and_return_string(rows[2].lower())\n",
    "    \n",
    "    if s in termsList and o in termsList:\n",
    "        count +=1\n",
    "        presentList.append([s,p,o])\n",
    "    elif s in termsList or o in termsList:\n",
    "        count1 +=1\n",
    "        solist.append([s,p,o])\n",
    "        \n",
    "    if s in termsList1 and o in termsList1:\n",
    "        count2 +=1\n",
    "        npresentList.append([s,p,o])\n",
    "    elif s in termsList1 or o in termsList1:\n",
    "        count3 +=1\n",
    "        nsolist.append([s,p,o])\n",
    "          \n",
    "        \n",
    "print(count, count1, count2, count3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(solist)\n",
    "df2.to_csv(\"/Users/ngautam/Desktop/finalData/init.csv\",index=False,header=None,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(presentList)\n",
    "df2.to_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/withoutdup/subjectobject_CUIs.tsv\",index=False,header=None,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd24c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Desktop/data.csv\", header=None, sep=\"\\t\")\n",
    "with open(\"/Users/ngautam/Desktop/out.txt\", \"w\") as file:\n",
    "    for index,rows in data.iterrows():\n",
    "        string = rows[0].strip('\"')\n",
    "        file.write(string+\"\\n\"+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9aac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = [\"Enterococci in water\",\n",
    "                    \"Fecal Coliform in water\",\n",
    "                    \"E.coli in marine water\",\n",
    "                    \"Fecal Indicator Bacteria in water\",\n",
    "                    \"E coli in water\",\n",
    "                    \"Water quality indicators\",\n",
    "                    \"Enteric Viruses\",\n",
    "                    \"Enteric infections from beaches\",\n",
    "                    \"Gastrointestinal Illness and beaches\",\n",
    "                    \"Beach water contamination\",\n",
    "                    \"Beach outbreaks\",\n",
    "                    \"Waterborne infections in divers\",\n",
    "                    \"Marine water quality\",\n",
    "                    \"Marine water point source pollution\",\n",
    "                    \"Quantitative microbial risk assessment and water\",\n",
    "                    \"Weather and waterborne illness\",\n",
    "                    \"Non cholera vibrio outbreaks\",\n",
    "                    \"Pfiesteria infection\",\n",
    "                    \"Red tides and illness\",\n",
    "                    \"Harmful algal blooms and illness\",\n",
    "                    \"Coastal water pollution\",\n",
    "                    \"Water salinity and illness\",\n",
    "                    \"Water characteristics and illness\",\n",
    "                    \"Tides and illness\",\n",
    "                    \"Harmful algal bloom\"]\n",
    "prev_keywords = []\n",
    "for k in key:\n",
    "    k = k.lower()\n",
    "    prev_keywords.append(k)\n",
    "    \n",
    "    if k in termsList:\n",
    "        print(k)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Keywords not in abstract list']  = pd.Series(termsList)\n",
    "df['Abstract Keywords List']  = pd.Series(prev_keywords)\n",
    "df['CUIS covered in SemMedDB']  = pd.Series(termsList)\n",
    "df['CUIS not covered in SemMedDB']  = pd.Series(termsList1)\n",
    "\n",
    "\n",
    "df.to_csv(\"~/Desktop/info.tsv\",index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e814de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For extracting triples from keywords abstracts\n",
    "\n",
    "def extract_trip_from_abstract(filename):\n",
    "    dataset = []\n",
    "    garbagge = []\n",
    "    fulldata = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            if (\"|relation|\") in line:\n",
    "                texts = line.split(\"|\")\n",
    "                if len(texts)  > 10:\n",
    "                    s = texts[3]\n",
    "                    p = texts[8]\n",
    "                    o = texts[10]\n",
    "                    if [s,p,o] not in dataset:\n",
    "                        if s != \"\" and p != \"\" and o != \"\":\n",
    "                            dataset.append([s,p,o])\n",
    "                    fulldata.append([s,p,o])\n",
    "                else:\n",
    "                    garbagge.append(texts)\n",
    "                    \n",
    "    return fulldata, dataset, garbagge\n",
    "\n",
    "\n",
    "# f, d, g = extract_trip_from_abstract(\"/Users/ngautam/Desktop/a.txt\")\n",
    "# df = pd.DataFrame(d)\n",
    "# df.to_csv(\"~/Desktop/algal-triples.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "   \n",
    "\n",
    "f, d, g = extract_trip_from_abstract(\"/Users/ngautam/Desktop/Abstract-related/fullkey.txt\")\n",
    "f1, d1,g1 = extract_trip_from_abstract(\"/Users/ngautam/Desktop/Abstract-related/nlmout.txt\")\n",
    "f2, d2,g2 = extract_trip_from_abstract(\"/Users/ngautam/Desktop/Abstract-related/algalout.txt\")\n",
    "\n",
    "dataset = d + d1 + d2\n",
    "fulldataset = f + f1 + f2\n",
    "garbagge = g + g1 + g2\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(\"~/Desktop/Abstract-related/abstract-triples/withoutduplicates.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "   \n",
    "df1 = pd.DataFrame(garbagge)\n",
    "df1.to_csv(\"~/Desktop/Abstract-related/abstract-triples/gar.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "\n",
    "df2 = pd.DataFrame(fulldataset)\n",
    "df2.to_csv(\"~/Desktop/Abstract-related/abstract-triples/fulldata.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c112f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine abstract and semMeddb triples for stats\n",
    "\n",
    "data1 = pd.read_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/withoutdup/abs.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "data2 = pd.read_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/withoutdup/sem.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "combD = []\n",
    "for index,rows in data1.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    combD.append([s,p,o])\n",
    "    \n",
    "    \n",
    "count = 0\n",
    "for index,rows in data2.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    if [s,p,o] not in combD:\n",
    "        combD.append([s,p,o])\n",
    "    else:\n",
    "        count +=1\n",
    "        \n",
    "print(\"Duplicate triples in both files:\", count)\n",
    "print(len(data1), len(data2), len(combD))\n",
    "        \n",
    "df = pd.DataFrame(combD)\n",
    "df.to_csv(\"/Users/ngautam/Desktop/Abstract-related/abs-sem-comb/withoutdup/combined.tsv\",index=False,header=None,sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c7d23bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Weather and waterborne illness 26\n",
      "2 Harmful algal blooms and illness 240\n",
      "3 Quantitative microbial risk assessment and water 889\n",
      "4 Fecal Indicator Bacteria in water 2873\n",
      "5 Coastal water pollution 13509\n",
      "6 Enteric infections from beaches 23\n",
      "7 Pfiesteria infection 28\n",
      "8 E.coli in marine water 828\n",
      "9 Marine water quality 25859\n",
      "10 Water quality indicators 22886\n",
      "11 Marine water point source pollution 816\n",
      "12 Waterborne infections in divers 7\n",
      "13 Red tides and illness 80\n",
      "14 Non cholera vibrio outbreaks 458\n",
      "15 Beach outbreaks 464\n",
      "16 Water characteristics and illness 573\n",
      "17 Fecal Coliform in water 3050\n",
      "18 Gastrointestinal Illness and beaches 107\n",
      "19 Beach water contamination 1039\n",
      "20 Enterococci in water 4719\n",
      "21 Water salinity and illness 87\n",
      "22 Enteric Viruses 7301\n",
      "23 Tides and illness 111\n",
      "24 E coli in water 24256\n",
      "25 Harmful algal bloom 7296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas\n",
    "import os, glob\n",
    "import time\n",
    "import csv\n",
    "import argparse \n",
    "\n",
    "url_list = set()\n",
    "rootdir = \"/Users/ngautam/Desktop/Abstract-related/backup\" +  \"/*\"\n",
    "\n",
    "top = 0 \n",
    "for filename in glob.iglob(rootdir, recursive=True):\n",
    "    if os.path.isfile(filename):\n",
    "#         print(filename)\n",
    "        count = 0 \n",
    "        pd = pandas.read_csv(filename, header = None)\n",
    "\n",
    "        for row in pd.itertuples():\n",
    "            key = row[2]\n",
    "            data = row[4]\n",
    "            url = row[3]\n",
    "            if data:\n",
    "                count +=1\n",
    "                if url not in url_list:\n",
    "                    url_list.add(url)\n",
    "        \n",
    "        top +=1\n",
    "        print(top, key,count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b3995d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75628\n",
      "102287\n"
     ]
    }
   ],
   "source": [
    "#Extracting sentences corresponding to entities from Abstracts -1 \n",
    "\n",
    "\n",
    "data = pd.read_csv(\"~/Desktop/finalData/david1trip.tsv\",header=None, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "final = []\n",
    "for index,rows in data.iterrows():\n",
    "    s = rows[0]\n",
    "    p = rows[1]\n",
    "    o = rows[2]\n",
    "    final.append([s,p,o])\n",
    "    \n",
    "print(len(final))\n",
    "\n",
    "def extract_sentence_from_abstract(filename):\n",
    "    dataset = []\n",
    "    tripset = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        for index, line in enumerate(data):\n",
    "            if index < len(data) - 2:\n",
    "                \n",
    "                nextLine = data[index+2]\n",
    "                \n",
    "                if (\"|relation|\") in nextLine and (\"|relation|\") not in line:\n",
    "                    \n",
    "                    texts = nextLine.split(\"|\")\n",
    "                    textid = texts[0]\n",
    "                    \n",
    "                    sentence = line.split(textid)\n",
    "                    if len(sentence) > 1:\n",
    "                        sen = sentence[1].split(\"\\n\")[0]\n",
    "                        dataset.append(sen.lower())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "d2 = extract_sentence_from_abstract(\"/Users/ngautam/Desktop/Abstract-related/algalout.txt\")\n",
    "d = extract_sentence_from_abstract(\"/Users/ngautam/Desktop/Abstract-related/fullkey.txt\")\n",
    "d1 = extract_sentence_from_abstract(\"/Users/ngautam/Desktop/Abstract-related/nlmout.txt\")\n",
    "\n",
    "dataset = d + d1 + d2\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "e99a7b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "22061\n"
     ]
    }
   ],
   "source": [
    "#Extracting sentences corresponding to entities from Abstracts -2 \n",
    "\n",
    "data = pd.read_csv(\"~/Desktop/Abstract-related/rowi.csv\",header=None, sep=\",\", low_memory=False)\n",
    "termsList = []\n",
    "for index,rows in data.iterrows():\n",
    "    terms = rows[0].lower()\n",
    "    present = rows[3].lower()\n",
    "    \n",
    "    if present == \"y\":\n",
    "        terms = terms.lower()\n",
    "        termsList.append(terms)\n",
    "\n",
    "print(len(termsList))\n",
    "\n",
    "sen_present = []\n",
    "for item in dataset:\n",
    "    for terms in termsList:\n",
    "        if terms in item:\n",
    "            if item not in sen_present:\n",
    "                sen_present.append(item)\n",
    "            \n",
    "print(len(sen_present))\n",
    "\n",
    "df = pd.DataFrame(sen_present)\n",
    "df.to_csv(\"~/Desktop/Abstract-related/abs-sentences/sentences-abs.txt\",index=False,header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
