#### The folder contains abstracts downloaded from WebOfScience and National Library of Medicine. 
#### These abstracts are used to fine-tune the BERT and RoBERTa models with the domain specific knowledge such that they can be further fine-tuned for Knowledge Graph Completion tasks.

## For documentation on fine-tuning transformer models see more information in the official transformers website.
