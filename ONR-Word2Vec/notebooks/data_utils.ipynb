{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad998c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import get_english_tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e27b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "from utils import build_vocab\n",
    "import torch\n",
    "class PDFDataset(Dataset):\n",
    "    \"\"\"PDF dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the coverted text files.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.filenames = list(filter(\n",
    "                                lambda file: not file.startswith(\"._\"), #macos adds some meta files, this removes them\n",
    "                                os.listdir(root_dir)\n",
    "                                ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.join(self.root_dir,self.filenames[idx])\n",
    "        with open(filename,\"r\") as f:\n",
    "            sample = unidecode(\" \".join([x.strip() for x in f.readlines()]))\n",
    "        return sample\n",
    "    \n",
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"\n",
    "    CBOW dataset.\n",
    "        This dataset uses the PDFDataset and creates context-token pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,tokenizer,vocab=None,window=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the coverted text files.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.pdf_dataset = PDFDataset(root_dir)\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = build_vocab(self.pdf_dataset,tokenizer)\n",
    "        self.text_pipeline = lambda x: self.vocab(tokenizer(x))\n",
    "        self.data=[]\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        for text in self.pdf_dataset:\n",
    "            text_tokens_ids = self.text_pipeline(text)\n",
    "\n",
    "            if len(text_tokens_ids) < self.window * 2 + 1:\n",
    "                continue\n",
    "\n",
    "            for idx in range(len(text_tokens_ids) - self.window * 2):\n",
    "                token_id_sequence = text_tokens_ids[idx : (idx + self.window * 2 + 1)]\n",
    "                output = token_id_sequence.pop(self.window)\n",
    "                input_ = token_id_sequence\n",
    "                self.data.append([input_,output])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context,token = self.data[idx]\n",
    "        return (torch.tensor(context,dtype=torch.long),token)\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SkipGramDataset dataset.\n",
    "        This dataset uses the PDFDataset and creates context-token pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,tokenizer,vocab=None,window: int =4,min_word_frequency: int = 50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the coverted text files.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.pdf_dataset = PDFDataset(root_dir)\n",
    "        if vocab:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = build_vocab(self.pdf_dataset,tokenizer, min_word_frequency)\n",
    "        self.text_pipeline = lambda x: self.vocab(tokenizer(x))\n",
    "        self.data=[]\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        for text in self.pdf_dataset:\n",
    "            text_tokens_ids = self.text_pipeline(text)\n",
    "\n",
    "            if len(text_tokens_ids) < self.window * 2 + 1:\n",
    "                continue\n",
    "\n",
    "            for idx in range(len(text_tokens_ids) - self.window * 2):\n",
    "                token_id_sequence = text_tokens_ids[idx : (idx + self.window * 2 + 1)]\n",
    "                input_ = token_id_sequence.pop(self.window)\n",
    "                outputs = token_id_sequence\n",
    "\n",
    "                for output in outputs:\n",
    "                    self.data.append([input_, output])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context,token = self.data[idx]\n",
    "        return (torch.tensor(context,dtype=torch.long),token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8cc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(root_dir = \"../data\",tokenizer = get_english_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(root_dir = \"../data\",tokenizer = get_english_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a6890",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
